import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


import torch
import torch.nn as nn
import math

# å®šä¹‰Diceç±»ï¼Œç”¨äºåœ¨å‰å‘ä¼ æ’­ä¸­å®ç°ç‰¹å®šçš„åŠ æƒæ“ä½œ
class Dice(nn.Module):
    """
    Diceç±»åˆå§‹åŒ–æ–¹æ³•ã€‚
    åˆå§‹åŒ–æ—¶ï¼Œé™¤äº†è°ƒç”¨nn.Moduleçš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¿˜å®šä¹‰äº†ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°alphaã€‚
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–Diceç±»ã€‚
        """
        super(Dice, self).__init__()  # è°ƒç”¨nn.Moduleçš„åˆå§‹åŒ–æ–¹æ³•
        self.alpha = nn.Parameter(torch.zeros((1, )))  # åˆå§‹åŒ–alphaå‚æ•°ä¸º0ï¼Œä½œä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°
        
    def forward(self, x):
        """
        å®šä¹‰å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚
        
        å‚æ•°:
        x: è¾“å…¥çš„å¼ é‡ã€‚
        
        è¿”å›:
        åŠ æƒåçš„è¾“å…¥å¼ é‡ã€‚
        """
        avg = x.mean(dim=0) # è®¡ç®—å‰ä¸€å±‚è¾“å‡ºçš„å‡å€¼
        std = x.std(dim=0) # è®¡ç®—å‰ä¸€å±‚è¾“å‡ºçš„æ ‡å‡†å·®
        norm_x = (x - avg) / std # è®¡ç®—å½’ä¸€åŒ–åçš„è¾“å…¥å¼ é‡
        p = torch.sigmoid(norm_x + 1e-8)

        return x.mul(p) + self.alpha * x.mul(1 - p)

# å®šä¹‰NeighborAggregationç±»ï¼Œç”¨äºé‚»å±…èŠ‚ç‚¹çš„èšåˆæ“ä½œ
class NeighborAggregation(nn.Module):
    """
    é‚»å±…èšåˆç±»ï¼Œç”¨äºå®ç°åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„èŠ‚ç‚¹èšåˆæ“ä½œã€‚

    å‚æ•°:
    embed_dim (int): åµŒå…¥ç»´åº¦ï¼Œé»˜è®¤ä¸º8ã€‚
    hidden_dim (int): éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤ä¸º8ã€‚
    """
    def __init__(self, embed_dim=8, hidden_dim=8):
        super(NeighborAggregation, self).__init__()
        # å®šä¹‰çº¿æ€§å˜æ¢çŸ©é˜µï¼Œç”¨äºå°†è¾“å…¥çš„åµŒå…¥å‘é‡ï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼ï¼‰å˜æ¢åˆ°éšç©ºé—´
        self.Q_w = nn.Linear(embed_dim, hidden_dim, bias=False)  # æŸ¥è¯¢å‘é‡çš„çº¿æ€§å˜æ¢
        self.K_w = nn.Linear(embed_dim, hidden_dim, bias=False)  # é”®å‘é‡çš„çº¿æ€§å˜æ¢
        self.V_w = nn.Linear(embed_dim, hidden_dim, bias=False)  # å€¼å‘é‡çš„çº¿æ€§å˜æ¢
        
        # è®¡ç®—ç¼©æ”¾å› å­ï¼Œç”¨äºç¼©æ”¾ç‚¹ç§¯ç»“æœï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸
        self.trans_d = math.sqrt(hidden_dim) 
        
        # å®šä¹‰softmaxå‡½æ•°ï¼Œç”¨äºè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†
        self.get_score = nn.Softmax(dim=-1) 

    def forward(self, query, key):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ï¼Œè®¡ç®—æ³¨æ„åŠ›æœºåˆ¶ä¸‹çš„é‚»å±…èŠ‚ç‚¹èšåˆç»“æœã€‚

        å‚æ•°:
        query (Tensor): æŸ¥è¯¢å‘é‡ã€‚
        key (Tensor): å…³é”®å‘é‡ã€‚

        è¿”å›:
        Tensor: èšåˆåçš„ç»“æœå‘é‡ã€‚
        """
        # å¯¹æŸ¥è¯¢å‘é‡è¿›è¡Œçº¿æ€§å˜æ¢
        trans_Q = self.Q_w(query) # (1024 x 21 x 20)ï¼Œæ˜¯å°†query(1024 Ã— 21 Ã— 10ï¼‰æ˜ å°„åˆ°éšç©ºé—´ (1024 x 21 x 20)
        # å¯¹å…³é”®å‘é‡è¿›è¡Œçº¿æ€§å˜æ¢
        trans_K = self.K_w(key) # (1024 x 1 x 20)
        # å¯¹æŸ¥è¯¢å‘é‡è¿›è¡Œç¬¬äºŒæ¬¡çº¿æ€§å˜æ¢ï¼Œå¾—åˆ°å€¼å‘é‡
        trans_V = self.V_w(query) # (1024 x 21 x 20)
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ŒåŸºäºå˜æ¢åçš„æŸ¥è¯¢å’Œå…³é”®å‘é‡çš„å†…ç§¯
        score = self.get_score(torch.bmm(trans_Q, torch.transpose(trans_K, 1, 2)) / (self.trans_d)) # (1024 x 21 x 1)ï¼Œ1024ä¸ªç”¨æˆ·ï¼Œæ¯ä¸ªç”¨æˆ·å¯¹21ä¸ªç‰©å“çš„æ³¨æ„åŠ›å¾—åˆ†
        # æ ¹æ®æ³¨æ„åŠ›æƒé‡å¯¹å€¼å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°èšåˆç»“æœ
        answer = torch.mul(trans_V, score) # (1024 x 21 x 20)
        return answer

# å®šä¹‰MacGNNç±»ï¼Œç”¨äºå®ç°å›¾ç¥ç»ç½‘ç»œæ¨¡å‹
class MacGNN(nn.Module):

    def __init__(self, field_dims, u_group_num, i_group_num, embed_dim, recent_len, tau=0.8, device='cpu'):
        super(MacGNN, self).__init__()
        # åˆå§‹åŒ–ç”¨æˆ·ã€ç‰©å“ã€ç±»åˆ«ç­‰çš„åµŒå…¥å±‚
        self.user_embed = nn.Embedding(field_dims[0], embed_dim)
        self.item_embed = nn.Embedding(field_dims[1], embed_dim)
        self.cate_embed = nn.Embedding(field_dims[2], embed_dim)
        # åˆå§‹åŒ–ç”¨æˆ·å’Œç‰©å“çš„å®è§‚åµŒå…¥å±‚
        self.u_macro_embed = nn.Embedding(u_group_num + 1, embed_dim)
        self.i_macro_embed = nn.Embedding(i_group_num + 1, embed_dim)
        # åˆå§‹åŒ–åµŒå…¥å±‚çš„æƒé‡
        # Xavieråˆå§‹åŒ–æ–¹æ³•ä½¿å¾—æƒé‡æœä»å‡åŒ€åˆ†å¸ƒ [âˆ’6/sqrt(ğ‘›_ğ‘–ğ‘›+ğ‘›_ğ‘œğ‘¢ğ‘¡),6/sqrt(ğ‘›_ğ‘–ğ‘›+ğ‘›_ğ‘œğ‘¢ğ‘¡)]
        torch.nn.init.xavier_uniform_(self.user_embed.weight.data)
        torch.nn.init.xavier_uniform_(self.item_embed.weight.data)
        torch.nn.init.xavier_uniform_(self.cate_embed.weight.data)
        torch.nn.init.xavier_uniform_(self.u_macro_embed.weight.data)
        torch.nn.init.xavier_uniform_(self.i_macro_embed.weight.data)
        # è®¾ç½®æ¸©åº¦å‚æ•°tau
        self.tau = tau
        # åˆå§‹åŒ–å…±äº«çš„é‚»å±…èšåˆæ¨¡å—
        self.u_shared_aggregator = NeighborAggregation(embed_dim, 2 * embed_dim)
        self.i_shared_aggregator = NeighborAggregation(embed_dim, 2 * embed_dim)        
        # self.u_shared_aggregator = NeighborAggregation(embed_dim, 128)
        # self.i_shared_aggregator = NeighborAggregation(embed_dim, 128)
        # è®¾ç½®ç”¨æˆ·å’Œç‰©å“çš„ç»„æ•°ã€æœ€è¿‘äº¤äº’åºåˆ—çš„é•¿åº¦ç­‰å‚æ•°
        self.u_group_num = u_group_num + 1
        self.i_group_num = i_group_num + 1
        self.recent_len = recent_len
        # åˆå§‹åŒ–softmaxå‡½æ•°ï¼Œç”¨äºè®¡ç®—å®è§‚æƒé‡
        self.macro_weight_func = nn.Softmax(dim=1) # dim=1 è¡¨ç¤ºå¯¹çŸ©é˜µçš„è¡Œè¿›è¡Œå½’ä¸€åŒ–ï¼Œdim=0 è¡¨ç¤ºå¯¹çŸ©é˜µçš„åˆ—è¿›è¡Œå½’ä¸€åŒ–
        # åˆå§‹åŒ–ç”¨äºç´¢å¼•çš„å¼ é‡ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.u_gruop_slice = torch.arange(self.u_group_num, requires_grad=False).to(device)
        self.i_gruop_slice = torch.arange(self.i_group_num, requires_grad=False).to(device)
        # åˆå§‹åŒ–å¤šå±‚æ„ŸçŸ¥å™¨æ¨¡å—ï¼Œç”¨äºæœ€åçš„é¢„æµ‹
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim * 14, 200),
            Dice(),
            nn.Linear(200, 80),
            Dice(),
            nn.Linear(80, 1)
        )

    def forward(self, x):
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡ä¸­ç”¨æˆ·IDå¯¹åº”çš„åµŒå…¥è¡¨ç¤º (1024 x 10)
        user_embedding = self.user_embed(x[:, 0]) 
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡ç”¨æˆ·çš„ä¸€é˜¶é‚»å±…ï¼ˆèšç±»ç‰©å“ï¼‰çš„è¾¹ (1024 Ã— 21)
        user_1ord_neighbor = x[:, 1: self.i_group_num + 1] 
        # è·å–ç”¨æˆ·2é˜¶é‚»å±…ï¼ˆèšç±»ç”¨æˆ·ï¼‰çš„è¾¹ (1024 Ã— 20)ï¼Œè¿™ä¸ª 20 æ˜¯è‡ªå·±è®¾å®šçš„èšç±»ç”¨æˆ·çš„ä¸ªæ•°
        user_2ord_neighbor = x[:, self.i_group_num + 1: self.i_group_num + self.u_group_num + 1] 
        # è·å–ç”¨æˆ·-ç‰©å“çš„äº¤äº’è®°å½• (1024 Ã— 20)
        user_recent = x[:, self.i_group_num + self.u_group_num + 1: self.i_group_num + self.u_group_num + self.recent_len + 1]
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡ç‰©å“çš„åµŒå…¥è¡¨ç¤º (1024 Ã— 10)
        item_embedding = self.item_embed(x[:, self.i_group_num + self.u_group_num + self.recent_len + 1])
        # è·å–ç‰©å“çš„ä¸€é˜¶é‚»å±…å’ŒäºŒé˜¶é‚»å±…
        item_1ord_neighbor = x[:, self.i_group_num + self.u_group_num + self.recent_len + 2: self.i_group_num + 2 * self.u_group_num + self.recent_len + 2]
        item_2ord_neighbor = x[:, self.i_group_num + 2 * self.u_group_num + self.recent_len + 2: 2 * self.i_group_num + 2 * self.u_group_num + self.recent_len + 2]
        # è·å–ç‰©å“-ç”¨æˆ·çš„äº¤äº’è®°å½•
        item_recent = x[:, 2 * self.i_group_num + 2 * self.u_group_num + self.recent_len + 2:]

        # å°†ä¸€ç»´çš„ç”¨æˆ·ç»„ç´¢å¼•å¤åˆ¶å¡«å……è‡³äºŒç»´ä¸ºæ¯ä¸ªç”¨æˆ·éƒ½åˆ†é…ä¸€ä¸ªç´¢å¼• (1024 Ã— 20)
        batch_u_gruop_slice = self.u_gruop_slice.expand(x.shape[0], self.u_group_num)
        # åŒç†åˆ›å»ºç‰©å“ç»„ç´¢å¼• (1024 Ã— 21)
        batch_i_gruop_slice = self.i_gruop_slice.expand(x.shape[0], self.i_group_num)

        # åˆ›å»ºæœ€è¿‘äº¤äº’çš„æ©ç ï¼Œç”¨äºåç»­åŠ æƒæ“ä½œ
        user_recent_mask = (user_recent > 0).float().unsqueeze(-1) # (1024 Ã— 20 Ã— 1)
        item_recent_mask = (item_recent > 0).float().unsqueeze(-1) # (1024 Ã— 20 Ã— 1)
        
        # è®¡ç®—å„ç±»é‚»å±…çš„æƒé‡
        user_1ord_weight = self.macro_weight_func(torch.log(user_1ord_neighbor.float() + 1) / self.tau).unsqueeze(-1) # (1024 Ã— 21 Ã— 1) self.macro_weight_func å¯¹è¡Œè¿›è¡Œå½’ä¸€åŒ–
        user_2ord_weight = self.macro_weight_func(torch.log(user_2ord_neighbor.float() + 1) / self.tau).unsqueeze(-1)
        item_1ord_weight = self.macro_weight_func(torch.log(item_1ord_neighbor.float() + 1) / self.tau).unsqueeze(-1)
        item_2ord_weight = self.macro_weight_func(torch.log(item_2ord_neighbor.float() + 1) / self.tau).unsqueeze(-1)

        # è·å–å„ç±»é‚»å±…çš„åµŒå…¥è¡¨ç¤º
        user_1ord_embedding = self.i_macro_embed(batch_i_gruop_slice) # (1024 Ã— 21 Ã— 10) 1024ä¸ªç”¨æˆ·ï¼Œæ¯ä¸ªç”¨æˆ·ä¸21ä¸ªç‰©å“ç»„æœ‰æˆ–è€…æ²¡æœ‰äº¤äº’ï¼Œæ¯ä¸ªç‰©å“ç»„10ä¸ªåµŒå…¥ç»´åº¦
        user_2ord_embedding = self.u_macro_embed(batch_u_gruop_slice) # (1024 Ã— 20 Ã— 10) 1024ä¸ªç”¨æˆ·ï¼Œæ¯ä¸ªç”¨æˆ·ä¸20ä¸ªç”¨æˆ·ç»„æœ‰æˆ–è€…æ²¡æœ‰äº¤äº’ï¼Œæ¯ä¸ªç”¨æˆ·ç»„10ä¸ªåµŒå…¥ç»´åº¦
        item_1ord_embedding = self.u_macro_embed(batch_u_gruop_slice) # (1024 Ã— 20 Ã— 10) 1024ä¸ªç‰©å“ï¼Œæ¯ä¸ªç‰©å“ä¸20ä¸ªç”¨æˆ·ç»„æœ‰æˆ–è€…æ²¡æœ‰äº¤äº’ï¼Œæ¯ä¸ªç”¨æˆ·ç»„10ä¸ªåµŒå…¥ç»´åº¦
        item_2ord_embedding = self.i_macro_embed(batch_i_gruop_slice) # (1024 Ã— 21 Ã— 10) 1024ä¸ªç‰©å“ï¼Œæ¯ä¸ªç‰©å“ä¸21ä¸ªç‰©å“ç»„æœ‰æˆ–è€…æ²¡æœ‰äº¤äº’ï¼Œæ¯ä¸ªç‰©å“ç»„10ä¸ªåµŒå…¥ç»´åº¦
        user_recent_embedding = self.item_embed(user_recent) # (1024 Ã— 20 Ã— 10) 1024ä¸ªç”¨æˆ·ï¼Œæ¯ä¸ªç”¨æˆ·æœ€è¿‘çš„20æ¡ä¸ç‰©å“ç»„äº¤äº’çš„è®°å½•ï¼Œæ¯ä¸ªç‰©å“10ä¸ªåµŒå…¥ç»´åº¦
        item_recent_embedding = self.user_embed(item_recent) # (1024 Ã— 20 Ã— 10) 1024ä¸ªç‰©å“ï¼Œæ¯ä¸ªç‰©å“æœ€è¿‘çš„20æ¡ä¸ç”¨æˆ·ç»„äº¤äº’çš„è®°å½•ï¼Œæ¯ä¸ªç”¨æˆ·10ä¸ªåµŒå…¥ç»´åº¦

        # ä½¿ç”¨é‚»å±…èšåˆæ¨¡å—å¯¹é‚»å±…åµŒå…¥è¿›è¡Œè½¬æ¢å’Œèšåˆ
        u_1ord_trans_emb = self.i_shared_aggregator(user_1ord_embedding, item_embedding.unsqueeze(1)) # (1024 Ã— 21 Ã— 20)
        u_2ord_trans_emb = self.u_shared_aggregator(user_2ord_embedding, user_embedding.unsqueeze(1)) # (1024 Ã— 20 Ã— 20)
        i_1ord_trans_emb = self.u_shared_aggregator(item_1ord_embedding, user_embedding.unsqueeze(1)) # (1024 Ã— 20 Ã— 20)
        i_2ord_trans_emb = self.i_shared_aggregator(item_2ord_embedding, item_embedding.unsqueeze(1)) # (1024 Ã— 21 Ã— 20)
        user_recent_trans_emb = self.i_shared_aggregator(user_recent_embedding, item_embedding.unsqueeze(1)) # (1024 Ã— 20 Ã— 20)
        item_recent_trans_emb = self.u_shared_aggregator(item_recent_embedding, user_embedding.unsqueeze(1)) # (1024 Ã— 20 Ã— 20)

        # æ ¹æ®æƒé‡å¯¹èšåˆç»“æœè¿›è¡ŒåŠ æƒæ±‚å’Œ
        user_1ord_ws = torch.mul(u_1ord_trans_emb, user_1ord_weight).sum(dim=1) # (1024 Ã— 20)
        user_2ord_ws = torch.mul(u_2ord_trans_emb, user_2ord_weight).sum(dim=1) # (1024 Ã— 20)
        item_1ord_ws = torch.mul(i_1ord_trans_emb, item_1ord_weight).sum(dim=1)
        item_2ord_ws = torch.mul(i_2ord_trans_emb, item_2ord_weight).sum(dim=1)
        user_recent_ws = torch.mul(user_recent_trans_emb, user_recent_mask).sum(dim=1)
        item_recent_ws = torch.mul(item_recent_trans_emb, item_recent_mask).sum(dim=1)

        # å°†æ‰€æœ‰ä¿¡æ¯è¿æ¥èµ·æ¥ï¼Œè¾“å…¥åˆ°å¤šå±‚æ„ŸçŸ¥å™¨ä¸­è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
        concated = torch.hstack([user_embedding, user_1ord_ws, user_2ord_ws, user_recent_ws, item_embedding, item_1ord_ws, item_2ord_ws,  item_recent_ws]) # (1024 Ã— 140)
        output = self.mlp(concated)
        # è¾“å‡ºç»“æœï¼Œä½¿ç”¨sigmoidå‡½æ•°å°†ç»“æœé™åˆ¶åœ¨0åˆ°1ä¹‹é—´
        output = torch.sigmoid(output)
        return output